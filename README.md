# Generative-Adversarial-Networks-GANs-Project
Our goal in the project is to write a text2image algorithm and create a new image according to the desired text. We implemented text embedding by creating an embedding matrix according to our own dictionary by using the pre-trained ``glove`` vectors we received as text, downloaded from kaggle with the name ‘glove.6B.200d.txt’[1].
Essentially, our methodology was to obtain the desired image with a Stacked GAN model given the texts. Stacked GAN’s concatenate two GAN algorithms called as ‘stage 1’ and ‘stage 2’, where in the first stage, low resolution images are obtained with the stage 1 generator, and by feeding the generated images to stage 2, the resolution is increased and detailed real-looking images are obtained. However, the process of training and coding both stages was cumbersome, hence we’ve implemented the stage 1 only. As a note, this is our only model that we managed to get some results. Different types of GAN’s like DcGAN, or other methods such as stable diffusion are also used for text2image applications which are based on deep learning .
The importance of this project is to create an artificial algorithm that thinks like humans and designs images that people want. We use visuals in many fields from marketing to technology and one of the cornerstones of these marketing strategies is texts. It will be much more effective to design a visual design using a text which will attract users and what they want to see according to their data. In addition, visual arts gain a different dimension with the use of algorithms in this way. Our algorithm, GANs, are used in this aspect. A brief overview of the whole process will be given below and will be detailed in the later sections.
<img width="1180" alt="Screenshot 2023-01-23 at 16 25 09" src="https://user-images.githubusercontent.com/59516214/214050758-2e4326bb-7414-4a27-a503-ae26e1183de1.png">

We’ve downloaded the text data from the given urls with a function using the operating systems library to communicate with our device in the beginning. Before giving input to the generator function, we need textual descriptions for getting our text databases to the generator side. Then, we need to convert those textual descriptions to text embedding. Text embedding simply means a vector of a real number which we want to convert a sentence into a vector of real number of fixed length. If we want to evaluate the texts clearly on GAN architecture, we need to make this operation. Then, for the final step for generator input, we need to concatenate embedding with a random noise vector.
After the generator input is obtained, we need to obtain the generator output that would be inputted to the discriminator. Generally, the aim of the generator is generating fake images from random noise. But, for our function, we made our “generator” based on the text embeddings. Because we wanted to generate text2image, we needed to make a relative image of texts.. We used upsampling and convolutional layers for building the generator.
Finally, to complete our GAN stage, we need to use a discriminator. Before explaining how a discriminator works and how it trains our images, we need to explain what discriminators’ inputs and outputs are. On the input side, we have two different types. We have images which are taken from either the dataset or from the generator and text which is related to images. The output of the discriminator aims to give zeros and ones because the discriminator is a training function of the generator. Its method of classifying outputs is that if our output is zero then our image is fake. Also, when our output is one, it means that our image is real. Since we need to look over around 80.000 images on datasets, we diminished the resolution of images to process the images more easily to 64*64*3 rgb format. During the coding of discriminator, we mainly used CNN classifier architecture which fits the problem well since we are classifying images.
All these steps are expressed in a more detailed fashion in the later section, but the methodology and planning process is shared here. Since we implemented stage 1 only, we expected the image quality would be low, for Stacked GAN architectures, images are turned to high quality at stage 2. However, we expected to receive related images with the captions.
