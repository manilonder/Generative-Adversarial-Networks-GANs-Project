{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqfkvEmgg5NU"
      },
      "outputs": [],
      "source": [
        "# Batchsizes, epoch number, training data number are all reduced for displaying purposes # Importing the required modules\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from struct import unpack\n",
        "import h5py\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt import tensorflow as tf\n",
        "import random\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import keras\n",
        "from keras import Input, Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.layers import Dense, LeakyReLU, BatchNormalization, ReLU, Reshape, UpSampling2D, Conv2D, Activation, \\\n",
        "concatenate, Flatten, Lambda, Concatenate, add, Input, LSTM, GRU, RNN, Embedding, Multiply, TimeDistributed, Bidirectional, RepeatVector, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from matplotlib import pyplot as plt\n",
        "from keras_preprocessing.sequence import pad_sequences from keras.utils import to_categorical\n",
        "from keras.models import Model, Sequential\n",
        "# path names can be changed for downloading the data somewhere else\n",
        "datTrainFileName = \"eee443_project_dataset_train.h5\"\n",
        "\n",
        " imTrainDirect = \"train_images/\" feaTrainDirect = \"train_tupled_data\"\n",
        "datTestFileName = \"eee443_project_dataset_test.h5\" imTestDirect = \"test_images/\"\n",
        "feaTestDirect = \"test_tupled_data\"\n",
        "# Functions to obtain the data\n",
        "def imgDown(data, namPath):\n",
        "# In this function, we downloaded the images from the given url\n",
        "tim = time.time() i=0\n",
        "if not os.path.exists(namPath): os.makedirs(namPath)\n",
        "for url in data:\n",
        "url = url.decode()\n",
        "name = url.split(\"/\")[-1].strip()\n",
        "path = os.path.join(namPath, name)\n",
        "if not os.path.exists(path):\n",
        "hed = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'}\n",
        "resp = requests.get(url, stream=True)\n",
        "# check if data is obttained successfully if resp.status_code == 200:\n",
        "with open(path, 'wb') as outfile: outfile.write(resp.content)\n",
        "# prints affirmation at each 1000 iterations if i % 1000 == 1:\n",
        "timRes = time.time() - tim\n",
        "timPerIter = timRes/i\n",
        "print(\"Total passed minute is: {:.2f} . There is {:.2f} seconds per iteration. Our iteration\n",
        "is: {}\".format(timRes/60, timPerIter, i)) i += 1\n",
        "# Functions for data processing\n",
        "\n",
        " def StringFromCaptArry(captArry):\n",
        "# This function is used for prediction and demonstration\n",
        "captList = []\n",
        "capt = \"\"\n",
        "if(captArry.ndim == 1):\n",
        "captArry = np.expand_dims(captArry, axis=0)\n",
        "for caps in captArry:\n",
        "for NamWord in caps:\n",
        "if (NamWord == 'x_NULL_') or (NamWord == 'x_START_') or (NamWord == 'x_END_'):\n",
        "continue\n",
        "capt += NamWord + \" \"\n",
        "captList.append(capt.strip()) capt = \"\"\n",
        "return captList\n",
        "def captGet(namList, imid, cap, name): indexx = namList.index(name) + imid.min() return cap[np.where(imid == indexx)]\n",
        "def PreProcCreation(directImg, shuffle=False):\n",
        "def fileMethod(path):\n",
        "img = tf.io.read_file(path)\n",
        "img = tf.image.decode_jpeg(img, channels=3) img = tf.image.resize(img, (64, 64))\n",
        "return img\n",
        "def nameProcess(path):\n",
        "name = path.numpy().decode().split(\"\\\\\")[-1]\n",
        "\n",
        " return name\n",
        "def proc(path):\n",
        "name = tf.py_function(nameProcess, [path], tf.string) img = tf.py_function(fileMethod, [path], tf.float32) return (img, name)\n",
        "datFile = tf.data.Dataset.list_files(str(directImg) + \"*.jpg\", shuffle=shuffle) return datFile.map(lambda x: proc(x))\n",
        "def create_features(namFile, ims, namList, imid, cap, process_size = 250):\n",
        "length = 0\n",
        "with open(namFile, \"wb\") as outfile:\n",
        "for data in tqdm(ims.batch(process_size)):\n",
        "image = data[0]\n",
        "name = data[1].numpy()\n",
        "feature = image\n",
        "for i in range(feature.shape[0]):\n",
        "feat = feature[i]\n",
        "namm = name[i].decode()\n",
        "cpt = captGet(namList, imid, cap, namm)\n",
        "tp = (feat, cpt, namm) pickle.dump(tp, outfile)\n",
        "length += 1\n",
        "outfile.close() return length\n",
        "def installPickle(namFile):\n",
        "with open(namFile, \"rb\") as f: while True:\n",
        "try:\n",
        "yield pickle.load(f)\n",
        "\n",
        " except EOFError: break\n",
        "def dataGet(directFeat, url=None, imid=None, cap=None, directImg=None):\n",
        "# Main function that combines the captions and corresponding images\n",
        "len = -1\n",
        "if not os.path.isfile(directFeat):\n",
        "if not directImg:\n",
        "raise Exception(\"No image directory given. Enter image directory for feature\n",
        "extraction.\")\n",
        "listName = [u.split(\"/\")[-1].strip() for u in np.char.decode(url).tolist()] images = PreProcCreation(directImg)\n",
        "len = create_features(directFeat, images, listName, imid, cap)\n",
        "dset = tf.data.Dataset.from_generator(installPickle, args=[directFeat], output_types=(np.float32,np.int32, tf.string))\n",
        "if len == -1:\n",
        "len = dset.reduce(0, lambda x, _: x + 1).numpy()\n",
        "return dset, len\n",
        "def GetEmbed(w2ix):\n",
        "# This function creates the embedding matrix with our vocabulary\n",
        "# Load Glove vectors pathGlove = 'glove.6B.200d.txt'\n",
        "indxEmbed = {} # empty dictionary\n",
        "fil = open(pathGlove, encoding=\"utf-8\")\n",
        "for line in fil:\n",
        "val = line.split()\n",
        "\n",
        "\n",
        " word = val[0]\n",
        "coefficients = np.asarray(val[1:], dtype='float32') # if (word == 'startseq' or word == 'unk' ):\n",
        "# print(word)\n",
        "indxEmbed[word] = coefficients fil.close()\n",
        "print('Our total vector of word is: %s' % len(indxEmbed))\n",
        "dimEmbed = 200 totVocab = 1004\n",
        "# Get 200-dim dense vector for each of the 10000 words in out vocabulary mtxEmbed = np.zeros((totVocab, dimEmbed))\n",
        "for word, i in w2ix.items():\n",
        "if (word == 'x_UNK_'): word = 'unk'\n",
        "vectEmbed = indxEmbed.get(word) if vectEmbed is None:\n",
        "print('There is no existing word for capturing')\n",
        "if vectEmbed is not None:\n",
        "# Words not found in the embedding index will be all zeros mtxEmbed[i] = vectEmbed\n",
        "return mtxEmbed\n",
        "def HotOne(X, size=1004):\n",
        "X = X -1\n",
        "onehott = np.zeros((X.shape[0], 0)) temp = np.zeros((X.shape[0], size)) tempp = np.arange(X.shape[0]) temp[tempp, X[:,0]] = 1\n",
        "onehott = np.hstack((onehott, temp))\n",
        "return onehott\n",
        "# Get the train data\n",
        "fill = h5py.File(datTrainFileName, 'r') for key in list(fill.keys()):\n",
        "\n",
        " print(key, \":\", fill[key][()].shape)\n",
        "captTrain = fill[\"train_cap\"][()] imidTrain = fill[\"train_imid\"][()] urlTrain = fill[\"train_url\"][()] codeWord = fill[\"word_code\"][()]\n",
        "dif = pd.DataFrame(codeWord) dif = dif.sort_values(0, axis=1) words = np.asarray(dif.columns)\n",
        "w2ixx = {}\n",
        "for i in range(len(words)):\n",
        "word = words[i] w2ixx[word] = i\n",
        "# To download the images, this code needs to be run but lasts long\n",
        "# If the data is already downloaded, path name can be changed accordingly.\n",
        "# save_ims(train_url,TRAIN_IMAGES_DIRECTORY )\n",
        "# This part is used for combining the images and their related captions using # the train_imid data, also the image resolutions are fixed to 64x64\n",
        "datTrain, datTrainLen = dataGet( feaTrainDirect, urlTrain, imidTrain, captTrain, imTrainDirect)\n",
        "print( \"Total data obtaining proportion is: {} / {} \".format(datTrainLen, len(urlTrain)) )\n",
        "# Similar with the training data, test data is combined and resolutions are fixed\n",
        "fill = h5py.File(datTestFileName, \"r\")\n",
        "captTest = fill[\"test_caps\"][()] imidTest = fill[\"test_imid\"][()] urlTest = fill[\"test_url\"][()]\n",
        "datTest, datTestLength = dataGet(feaTestDirect, urlTest, imidTest, captTest, imTestDirect) print( \"The proportion of data obtaining is: {} / {} \".format(datTestLength, len(urlTest), ))\n",
        "for d in datTrain.shuffle(500).take(1): feats = d[0]\n",
        "namImg= d[2].numpy().decode()\n",
        "\n",
        " capt = d[1].numpy()\n",
        "imgg = cv2.imread(imTrainDirect + namImg)\n",
        "imgg = cv2.cvtColor(imgg, cv2.COLOR_BGR2RGB) plt.imshow(imgg)\n",
        "plt.show()\n",
        "captt = StringFromCaptArry(words[capt])\n",
        "print(captt[0])\n",
        "print(feats.shape, capt.shape)\n",
        "for d in datTrain.shuffle(1000).take(1): feats = d[0].numpy()\n",
        "namImg= d[2].numpy().decode() capt = d[1].numpy()\n",
        "xxx = feats plt.imshow(xxx.astype('uint8'))\n",
        "# Creating the embedding matrix from glove vectors\n",
        "matrixEmbed = GetEmbed(w2ixx)\n",
        "# Functions for the GAN architecture\n",
        "def lossKL(corY, predY):\n",
        "u = predY[:, :128]\n",
        "sigLogarithm = predY[:, :128]\n",
        "totLoss = -sigLogarithm + .5 * (-1 + K.exp(2. * sigLogarithm) + K.square(u)) totLoss = K.mean(totLoss)\n",
        "return totLoss\n",
        "def LossCustGen(corY, predY):\n",
        "return K.binary_crossentropy(corY, predY)\n",
        "def ImgSave(img, path):\n",
        "#rgb image saving\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1, 1, 1) ax.imshow(img) ax.axis(\"off\") ax.set_title(\"Image\")\n",
        "\n",
        " plt.savefig(path) plt.close()\n",
        "def cGen(x):\n",
        "u = x[:, :128]\n",
        "sigmaLogaritmm = x[:, 128:]\n",
        "derivStandart = K.exp(sigmaLogaritmm)\n",
        "epsilon = K.random_normal(shape=K.constant((u.shape[1],), dtype='int32')) crr = derivStandart * epsilon + u\n",
        "return crr\n",
        "def caModelBuild(): # Cond\n",
        "totLayerInp = Input(shape=(3400,))\n",
        "x = Dense(256)(totLayerInp)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "mod = Model(inputs=[totLayerInp], outputs=[x]) return mod\n",
        "def CompresBuildEmbedModel():\n",
        "# Compressing the embedding matrix\n",
        "totLayerInp = Input(shape=(3400,)) x = Dense(128)(totLayerInp)\n",
        "x = ReLU()(x)\n",
        "mod = Model(inputs=[totLayerInp], outputs=[x]) return mod\n",
        "def stage1BuildGen():\n",
        "totLayerInp = Input(shape=(3400,))\n",
        "dns = Dense(256)(totLayerInp) sigmaLogU = LeakyReLU(alpha=0.2)(dns)\n",
        "crr = Lambda(cGen)(sigmaLogU)\n",
        "totLayerInp2 = Input(shape=(100,))\n",
        "InpGen = Concatenate(axis=1)([crr, totLayerInp2])\n",
        "dns = Dense(128 * 8 * 4 * 4, use_bias=False)(InpGen) dns = ReLU()(dns)\n",
        "dns = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(dns)\n",
        "dns = UpSampling2D(size=(2, 2))(dns)\n",
        "dns = Conv2D(512, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = ReLU()(dns)\n",
        "dns = UpSampling2D(size=(2, 2))(dns)\n",
        "dns = Conv2D(256, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = ReLU()(dns)\n",
        "dns = UpSampling2D(size=(2, 2))(dns)\n",
        "dns = Conv2D(128, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = ReLU()(dns)\n",
        "dns = UpSampling2D(size=(2, 2))(dns)\n",
        "dns = Conv2D(64, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = ReLU()(dns)\n",
        "dns = Conv2D(3, kernel_size=3, padding=\"same\", strides=1, use_bias=False)(dns) dns = Activation(activation='tanh')(dns)\n",
        "wrkStage1 = Model(inputs=[totLayerInp, totLayerInp2], outputs=[dns,sigmaLogU])\n",
        "return wrkStage1 def stage1BuildDisc():\n",
        "totLayerInp = Input(shape=(64, 64, 3))\n",
        "dns = Conv2D(64, (4, 4), padding='same', strides=2,\n",
        "input_shape=(64, 64, 3), use_bias=False)(totLayerInp) dns = LeakyReLU(alpha=0.2)(dns)\n",
        "dns = Conv2D(128, (4, 4), padding='same', strides=2, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = LeakyReLU(alpha=0.2)(dns)\n",
        "dns = Conv2D(256, (4, 4), padding='same', strides=2, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "dns = LeakyReLU(alpha=0.2)(dns)\n",
        "dns = Conv2D(512, (4, 4), padding='same', strides=2, use_bias=False)(dns) dns = BatchNormalization()(dns)\n",
        "\n",
        "\n",
        " dns = LeakyReLU(alpha=0.2)(dns) totLayerInp2 = Input(shape=(4, 4, 128)) InpComb = concatenate([dns, totLayerInp2])\n",
        "dns2 = Conv2D(64 * 8, kernel_size=1, padding=\"same\", strides=1)(InpComb)\n",
        "dns2 = BatchNormalization()(dns2) dns2 = LeakyReLU(alpha=0.2)(dns2) dns2 = Flatten()(dns2)\n",
        "dns2 = Dense(1)(dns2)\n",
        "dns2 = Activation('sigmoid')(dns2)\n",
        "modDisStage1 = Model(inputs=[totLayerInp, totLayerInp2], outputs=[dns2]) return modDisStage1\n",
        "def advModelBuild(ModGen, modDis): totLayerInp = Input(shape=(3400,)) totLayerInp2 = Input(shape=(100,)) totLayerInp3 = Input(shape=(4, 4, 128))\n",
        "dns, sigmaLogU = ModGen([totLayerInp, totLayerInp2])\n",
        "modDis.trainable = False\n",
        "corr = modDis([dns, totLayerInp3])\n",
        "modd = Model(inputs=[totLayerInp, totLayerInp2, totLayerInp3], outputs=[corr, sigmaLogU])\n",
        "return modd\n",
        "lenData = 30 #train_data_length trData = datTrain.take(lenData)\n",
        "lenValue = round(lenData * 0.15) trValue = lenData - lenValue\n",
        "datVal = trData.take(lenValue) datTr = trData.skip(lenValue)\n",
        "# Captions and Images are received from train and val data\n",
        "str = []\n",
        "strr = []\n",
        "for data in datTr:\n",
        "\n",
        "\n",
        " feat = data[0].numpy() cp = data[1].numpy()\n",
        "str.append(feat) strr.append(cp[0])\n",
        "sttr = []\n",
        "sttrr = []\n",
        "for data in datVal:\n",
        "featt = data[0].numpy() cpp = data[1].numpy()\n",
        "sttr.append(featt) sttrr.append(cpp[0])\n",
        "# Here, text embedding is done using the embedding matrix and it is flattened.\n",
        "relEmbed = []\n",
        "for i in range(trValue):\n",
        "emb = np.array(strr[i:i+1])\n",
        "em = np.transpose(emb)\n",
        "emm = HotOne(em)\n",
        "emmm = np.matmul(emm,matrixEmbed) relEmbed.append(emmm)\n",
        "bssEm = np.array(relEmbed)\n",
        "bssEm = bssEm.reshape((trValue,3400)) bssEm.shape\n",
        "datNum1 = lenValue relEmb1 = []\n",
        "for i in range(lenValue):\n",
        "emb1 = np.array(sttrr[i:i+1])\n",
        "em1 = np.transpose(emb1)\n",
        "emm1 = HotOne(em1)\n",
        "emmm1 = np.matmul(emm1,matrixEmbed) relEmb1.append(emmm1)\n",
        "bssEm1 = np.array(relEmb1)\n",
        "bssEm1 = bssEm1.reshape((lenValue,3400))\n",
        "bssEm1.shape,bssEm.shape\n",
        "trAr = np.array(str[0:trValue]) trEmbed = bssEm\n",
        "\n",
        " valAr = np.array(sttr[0:trValue]) valEmbed = bssEm1\n",
        "trAr.shape,valAr.shape\n",
        "# The training of the model\n",
        "if __name__ == '__main__':\n",
        "totImg = 64 totBatch = 4 dimensionZ = 100 LRgenST1 = 0.001 LRdistST1 = 0.001 LRdsST1 = 600 epochs = 1000 totCond = 128\n",
        "# Optimizers that we use for generator and discriminator Optdis = Adam(lr=LRdistST1, beta_1=0.5, beta_2=0.999) Optgen = Adam(lr=LRgenST1, beta_1=0.5, beta_2=0.999)\n",
        "# Networks implemented and build\n",
        "ModCA = caModelBuild() ModCA.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "disST1 = stage1BuildDisc() disST1.compile(loss='binary_crossentropy', optimizer=Optdis)\n",
        "genST1 = stage1BuildGen() genST1.compile(loss=\"mse\", optimizer=Optgen)\n",
        "modEmbedComp = CompresBuildEmbedModel() modEmbedComp.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
        "modAdver = advModelBuild(ModGen=genST1, modDis=disST1) modAdver.compile(loss=['binary_crossentropy', lossKL], loss_weights=[1, 2.0],\n",
        "optimizer=Optgen, metrics=None)\n",
        "# Fake and Real Labels\n",
        "labCor = np.ones((totBatch, 1), dtype=float) * 0.9\n",
        "\n",
        " labWro = np.zeros((totBatch, 1), dtype=float)\n",
        "for epoch in range(epochs):\n",
        "print(\"Please wait...\")\n",
        "print(\"Total Epoch:\", epoch)\n",
        "print(\"Total Batches:\", int(trAr.shape[0] / totBatch))\n",
        "LossGen = [] LossDis = []\n",
        "# Here is batch gradient descend totBatchh = int(trAr.shape[0] / totBatch) for index in range(totBatchh):\n",
        "print(\"Total Batch:{}\".format(index+1))\n",
        "# Batch data and noise,\n",
        "noiseZ = np.random.normal(0, 1, size=(totBatch, dimensionZ)) batchImg = trAr[index * totBatch:(index + 1) * totBatch] batchEmbed = trEmbed[index * totBatch:(index + 1) * totBatch] batchImg = (batchImg - 127.5) / 127.5\n",
        "# Forward pass through the generator network\n",
        "WroImg, _ = genST1.predict([batchEmbed, noiseZ], verbose=3)\n",
        "# Compressing the text embedding further\n",
        "embedComp = modEmbedComp.predict_on_batch(batchEmbed) embedComp = np.reshape(embedComp, (-1, 1, 1, totCond)) embedComp = np.tile(embedComp, (1, 4, 4, 1))\n",
        "RealLossDis = disST1.train_on_batch([batchImg, embedComp], np.reshape(labCor, (totBatch, 1)))\n",
        "FakeLossDis = disST1.train_on_batch([WroImg, embedComp], np.reshape(labWro, (totBatch, 1)))\n",
        "WrongLossDis = disST1.train_on_batch([batchImg[:(totBatch - 1)], embedComp[1:]], np.reshape(labWro[1:], (totBatch-1, 1)))\n",
        "LossD = 0.5 * np.add(RealLossDis, 0.5 * np.add(WrongLossDis, FakeLossDis))\n",
        "# Training of the generator network\n",
        "LossG = modAdver.train_on_batch([batchEmbed, noiseZ, embedComp],[K.ones((totBatch, 1)) * 0.9, K.ones((totBatch, 256)) * 0.9])\n",
        "LossDis.append(LossD) LossGen.append(LossG)\n",
        "\n",
        " print(\"Loss of D:{}\".format(LossD)) print(\"Loss of G:{}\".format(LossG))\n",
        "if epoch % 1 == 0:\n",
        "# Predict on Validation see the loss\n",
        "NoiseZZ = np.random.normal(0, 1, size=(totBatch, dimensionZ)) batchEmbed = valEmbed[0:totBatch]\n",
        "WroImg, _ = genST1.predict_on_batch([batchEmbed, NoiseZZ]) lossVal = modAdver.evaluate([batchEmbed, noiseZ,\n",
        "embedComp],[K.ones((totBatch, 1)) * 0.9, K.ones((totBatch, 256)) * 0.9]) print(\"Validation Loss:{}\".format(lossVal))\n",
        "# Save images\n",
        "for i, img in enumerate(WroImg[:10]):\n",
        "ImgSave(img, \"Printed Results/gen_{}_{}.png\".format(epoch, i))\n",
        "# Save the model in each epoch\n",
        "# stage1_gen.save_weights(\"stage1_gen.h5\") # stage1_dis.save_weights(\"stage1_dis.h5\")\n",
        "# summary of generator network genST1.summary()\n",
        "lenTest = 3000 #test_data_length datTest = datTest.take(lenTest)\n",
        "xRowColll = [] xRowwColll = [] for data in datTest:\n",
        "feattt = data[0].numpy() cpsss = data[1].numpy()\n",
        "xRowColll.append(feattt) xRowwColll.append(cpsss[0])\n",
        "realEmbbb = []\n",
        "for i in range(lenTest):\n",
        "embeddd = np.array(xRowwColll[i:i+1])\n",
        "emRowColll = np.transpose(embeddd)\n",
        "emRowwColll = HotOne(emRowColll)\n",
        "emRowwwColll = np.matmul(emRowwColll,matrixEmbed) realEmbbb.append(emRowwwColll)\n",
        "bassEmbeddd = np.array(realEmbbb)\n",
        "\n",
        "bassEmbeddd = bassEmbeddd.reshape((lenTest,3400)) bassEmbeddd.shape\n",
        "testX = np.array(str[0:lenTest]) testEmbeds = bssEm\n",
        "testX.shape,testEmbeds.shape\n",
        "btcEmbed1 = testEmbeds[0:totBatch]\n",
        "imgWro = genST1.predict_on_batch([btcEmbed1, NoiseZZ])\n",
        "cpExamp = np.array(strr)\n",
        "cpp = StringFromCaptArry(words[strr[0]])\n",
        "print(cpp)\n",
        "# needs to be trained, output won't give a good result imggg = imgWro[0]\n",
        "plt.imshow(imggg[0])"
      ]
    }
  ]
}